{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb61b65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from my_data_loader import try_gpu, load_data_fashion_mnist\n",
    "from my_model import my_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34146eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resnet18(num_classes, in_channels=1):\n",
    "    \"\"\"稍加修改的ResNet-18模型\"\"\"\n",
    "    def resnet_block(in_channels, out_channels, num_residuals, first_block=False):   \n",
    "        blk = []\n",
    "        for i in range(num_residuals):\n",
    "            if i == 0 and not first_block:\n",
    "                blk.append(d2l.Residual(in_channels, out_channels, use_1x1conv=True,strides=2))    \n",
    "            else:\n",
    "                blk.append(d2l.Residual(out_channels,out_channels))\n",
    "        return nn.Sequential(*blk)\n",
    "        \n",
    "    net = nn.Sequential(\n",
    "        nn.Conv2d(in_channels,64,kernel_size=3,stride=1,padding=1),\n",
    "        nn.BatchNorm2d(64),\n",
    "        nn.ReLU())  \n",
    "        \n",
    "    net.add_module(\"resnet_block1\", resnet_block(64,64,2,first_block=True))\n",
    "    net.add_module(\"resnet_block2\", resnet_block(64,128,2))\n",
    "    net.add_module(\"resnet_block3\", resnet_block(128,256,2))\n",
    "    net.add_module(\"resnet_block4\", resnet_block(256,512,2))\n",
    "    net.add_module(\"resnet_avg_pool\", nn.AdaptiveAvgPool2d((1,1)))\n",
    "    net.add_module(\"fc\", nn.Sequential(nn.Flatten(), nn.Linear(512, num_classes)))  \n",
    "\n",
    "    return net\n",
    "    \n",
    "net = resnet18(10)\n",
    "devices = d2l.try_all_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc9dab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练\n",
    "def train(net, num_gpus, batch_size, lr):\n",
    "    train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)\n",
    "    devices = [d2l.try_gpu(i) for i in range(num_gpus)]\n",
    "    \n",
    "    def init_weights(m):\n",
    "        if type(m) in [nn.Linear, nn.Conv2d]:\n",
    "            nn.init.normal_(m.weight, std=0.01)\n",
    "            \n",
    "    net.apply(init_weights)\n",
    "    # nn.DataParallel会的是X切开并行到各个GPU上，并行算梯度，然后loss加起来，它重新定义了net的forward函数\n",
    "    net = nn.DataParallel(net, device_ids=devices) # net会复制到每一个GPU上\n",
    "    trainer = torch.optim.SGD(net.parameters(),lr)\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    timer, num_epochs = d2l.Timer(), 10\n",
    "    animator = d2l.Animator('epoch','test acc', xlim=[1, num_epochs])  \n",
    "    for epoch in range(num_epochs):\n",
    "        net.train()\n",
    "        timer.start()\n",
    "        for X, y in train_iter:\n",
    "            trainer.zero_grad()\n",
    "            X, y = X.to(devices[0]), y.to(devices[0])\n",
    "            l = loss(net(X), y)\n",
    "            l.backward()\n",
    "            trainer.step()\n",
    "        timer.stop()\n",
    "        animator.add(epoch+1, (d2l.evaluate_accuracy_gpu(net, test_iter),))    \n",
    "    print(f'test acc: {animator.Y[0][-1]:.2f}, {timer.avg():.1f} sec/epoch'\n",
    "         f'on {str(devices)}')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
